{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "from torchvision import transforms\n",
    "\n",
    "sys.path.append(r'/home/declan/Documents/Projects/CS639-FinalProject/src/reconstruction/')\n",
    "from dataset import FaceDataset\n",
    "from policy import Policy\n",
    "from utils import StateObject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some test code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-50f34adc5e32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Test whether the agent works.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrec_errs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentropies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisited_idxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoded_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather_trajectory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m#log_probs, rec_errs, rewards, entropies, decoded, values, visited_idxes, decoded_all, _ = agent.gather_trajectory(state, eval_opts=None, pano_maps=pano_maps, opts=opts)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-b71bb9e19e79>\u001b[0m in \u001b[0;36mgather_trajectory\u001b[0;34m(self, state_object)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# probs and value correspond to the new transition, where the value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;31m# and action probabilities of the current state are estimated for PG update.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0maction_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0;31m#print(action_probs.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m#print(action_probs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Projects/CS639-FinalProject/src/reconstruction/policy.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, hidden)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;31m# Decode the whole image using the decoder.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mdecoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m    776\u001b[0m         return F.conv_transpose2d(\n\u001b[1;32m    777\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m             output_padding, self.groups, self.dilation)\n\u001b[0m\u001b[1;32m    779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tensor = torch.zeros([32, 3, 512, 512]) # Random input images. batch size=32.\n",
    "#action_probs = np.random.uniform(low=0, high=1, size=(32, 256)) # Random action probs.\n",
    "state_object = StateObject(tensor)   # Create the view object.\n",
    "#state_object.get_views(action_probs) # Get views corresponding to the max prob actions.\n",
    "\n",
    "#im, pro = state_object.get_view()\n",
    "#im, pro = torch.Tensor(im), torch.Tensor(pro)\n",
    "\n",
    "# Policy forward pass\n",
    "#policy_input = {'im': im, 'pro': pro}\n",
    "\n",
    "#tensor = torch.zeros([32, 3, 512, 512])\n",
    "#model = Policy()\n",
    "#probs, hidden, decoded = model(policy_input)\n",
    "\n",
    "# Test whether the agent works.\n",
    "agent = Agent()\n",
    "log_probs, rec_errs, rewards, entropies, decoded, visited_idxes, decoded_all, _ = agent.gather_trajectory(state_object)\n",
    "\n",
    "#log_probs, rec_errs, rewards, entropies, decoded, values, visited_idxes, decoded_all, _ = agent.gather_trajectory(state, eval_opts=None, pano_maps=pano_maps, opts=opts)\n",
    "# Backward pass\n",
    "agent.update_policy(rewards, log_probs, entropies, task_errs=None, values=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Batch_idx 0 Loss: 0.21\n",
      "Epoch 0 Batch_idx 1 Loss: 0.44\n",
      "Epoch 0 Batch_idx 2 Loss: 0.66\n",
      "Epoch 0 Batch_idx 3 Loss: 0.86\n",
      "Epoch 0 Batch_idx 4 Loss: 1.11\n",
      "Epoch 0 Batch_idx 5 Loss: 1.32\n",
      "Epoch 0 Batch_idx 6 Loss: 1.52\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-01b1cc0fc8c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mstate_object\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStateObject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrec_errs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentropies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisited_idxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoded_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather_trajectory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentropies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrec_errs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-74cce8455343>\u001b[0m in \u001b[0;36mgather_trajectory\u001b[0;34m(self, state_object)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;31m# ---- Observe the panorama ----\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;31m# At first iteration, action probs==None, so state_object returns a fixed view at (0,0).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpro\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpro\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpro\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Projects/CS639-FinalProject/src/reconstruction/utils.py\u001b[0m in \u001b[0;36mget_view\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minds\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mview_subarray_inds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0minds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0minds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mview_locs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mviews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 4"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "\n",
    "# Set up the Datasets and the DataLoaders.\n",
    "transforms_ = transforms.Compose([transforms.Resize(size=(512, 512)), transforms.ToTensor()])\n",
    "dataset = FaceDataset(transforms=transforms_)\n",
    "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Set up the model and the optimizer.\n",
    "agent = Agent()\n",
    "#optimizer = optim.Adam(model.parameters(), lr=.001)\n",
    "\n",
    "# Begin training model.\n",
    "n_epochs = 100\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_total_loss = 0\n",
    "    for batch_idx, data in enumerate(data_loader):\n",
    "        images, _ = data\n",
    "        state_object = StateObject(images)\n",
    "        log_probs, rec_errs, rewards, entropies, decoded, visited_idxes, decoded_all, _ = agent.gather_trajectory(state_object)\n",
    "        agent.update_policy(rewards, log_probs, entropies)\n",
    "        loss = rec_errs\n",
    "        epoch_total_loss += sum(loss)/len(loss)\n",
    "        print('Epoch {} Batch_idx {} Loss: {:.2f}'.format(epoch, batch_idx, epoch_total_loss.detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \"\"\"\n",
    "    This agent implements the policy from Policy class and uses REINFORCE / Actor-Critic for policy improvement\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Agent, self).__init__()\n",
    "        self.policy = Policy()\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=.01)\n",
    "        self.iscuda = False\n",
    "        \n",
    "        # Maybe not necessary.\n",
    "        self.R_avg = 0\n",
    "        self.avg_count = 0\n",
    "        self.reward_scale = .1\n",
    "        self.lambda_entropy = .01\n",
    "        self.selection_type = 'greedy'\n",
    "\n",
    "\n",
    "    def gather_trajectory(self, state_object):\n",
    "        \"\"\"\n",
    "        gather_trajectory gets an observation, updates it's belief of the state, decodes the\n",
    "        panorama and takes the next action. This is done repeatedly for T time steps.\n",
    "        Note:\n",
    "        eval_opts are provided only during testing\n",
    "        \"\"\"\n",
    "\n",
    "        # Setup variables to store trajectory information.\n",
    "        self.T = 10\n",
    "        rewards = []\n",
    "        log_probs = []\n",
    "        rec_errs = []\n",
    "        entropies = []\n",
    "        self.avg_count = 0\n",
    "        hidden = None\n",
    "        visited_idxes = []\n",
    "        batch_size = state_object.batch_size\n",
    "        decoded_all = []\n",
    "        actions_taken = torch.zeros(batch_size, self.T-1)\n",
    "        action_probs = None\n",
    "        \n",
    "        for t in range(self.T):\n",
    "            #print('Iteration ', str(t))\n",
    "            # ---- Observe the panorama ----\n",
    "            # At first iteration, action probs==None, so state_object returns a fixed view at (0,0).\n",
    "            im, pro = state_object.get_view(action_probs)\n",
    "            im, pro = torch.Tensor(im), torch.Tensor(pro)\n",
    "            \n",
    "            # Keep track of visited locations\n",
    "            visited_idxes.append(state_object.index)\n",
    "\n",
    "            # ---- Policy forward pass ----\n",
    "            policy_input = {'im': im, 'pro': pro}\n",
    "            \n",
    "            # Update if using CUDA.\n",
    "            if self.iscuda:\n",
    "                for var in policy_input:\n",
    "                    policy_input[var] = policy_input[var].cuda()\n",
    "            \n",
    "            # Otherwise use the CPU.\n",
    "            else:\n",
    "                for var in policy_input:\n",
    "                    policy_input[var] = Variable(policy_input[var])\n",
    "\n",
    "            # Note: decoded and hidden correspond to the previous transition\n",
    "            # probs and value correspond to the new transition, where the value\n",
    "            # and action probabilities of the current state are estimated for PG update.\n",
    "            action_probs, hidden, decoded = self.policy.forward(policy_input, hidden)\n",
    "\n",
    "            # Compute reconstruction loss (corresponding to the previous transition).\n",
    "            ###### NOTE: MAY BE APPROPRIATE TO USE A DIFFERENT LOSS FUNCTION HERE.\n",
    "            rec_err = F.mse_loss(decoded, state_object.images)\n",
    "            #self.optimizer.zero_grad()\n",
    "            #rec_err.backward(retain_graph=True)\n",
    "            #self.optimizer.step()\n",
    "            # \n",
    "\n",
    "            # Reconstruction reward is obtained only at the final step\n",
    "            # If there is only one step (T=1), then do not provide rewards\n",
    "            # Note: This reward corresponds to the previous action\n",
    "            if t < self.T-1 or t == 0:\n",
    "                reward = torch.zeros(batch_size)\n",
    "                if self.iscuda:\n",
    "                    reward = reward.cuda()\n",
    "            ##### I HAVE NO IDEA WHAT'S GOING ON HERE. #####\n",
    "            else:\n",
    "                reward = -rec_err.data # Disconnects reward from future updates\n",
    "                self.R_avg = (self.R_avg * self.avg_count + reward.sum())/(self.avg_count + batch_size)\n",
    "                self.avg_count += batch_size\n",
    "            if t > 0:\n",
    "                rewards[t-1] += reward\n",
    "                \n",
    "            # There are self.T reconstruction errors as opposed to self.T-1 rewards\n",
    "            rec_errs.append(rec_err)\n",
    "\n",
    "            # Sample action except for the last time step when only the selected view from previous step is used in aggregate.\n",
    "            if t < self.T - 1:\n",
    "                # Act based on the policy network\n",
    "                if self.selection_type == 'greedy':\n",
    "                    act = action_probs.multinomial(num_samples=1).data\n",
    "                else:\n",
    "                    # This works only while evaluating, not while training\n",
    "                    _, act = action_probs.max(dim=1)\n",
    "                    act = act.data.view(-1, 1)\n",
    "                \n",
    "                # Compute entropy\n",
    "                entropy = -(action_probs*((action_probs+1e-7).log())).sum(dim=1)\n",
    "                # Store log probabilities of selected actions (Advanced indexing).\n",
    "                log_prob = (action_probs[range(act.size(0)), act[:, 0]]+1e-7).log() # Save log_probs of choosing selected actions.\n",
    "\n",
    "                # This is the intrinsic reward corresponding to the current action\n",
    "                #### NOTE: COULD BE WRONG.\n",
    "                rewards.append(reward)\n",
    "                log_probs.append(log_prob)\n",
    "                entropies.append(entropy)\n",
    "\n",
    "        return log_probs, rec_errs, rewards, entropies, decoded, visited_idxes, decoded_all, actions_taken\n",
    "    \n",
    "    \n",
    "    def update_policy(self, rewards, log_probs, entropies, task_errs=None, values=None):\n",
    "        \"\"\"\n",
    "        This function will take the rewards, log probabilities and task-specific errors from\n",
    "        the trajectory and perform the parameter updates for the policy using REINFORCE.\n",
    "        INPUTS:\n",
    "            rewards: list of T-1 Tensors containing reward for each batch at each time step.\n",
    "            log_probs: list of T-1 logprobs Variables of each transition of batch.\n",
    "            task_errs: list of T error Variables for each transition of batch.\n",
    "            entropies: list of T-1 entropy Variables for each transition of batch.\n",
    "            values: list of T-1 predicted values Variables for each transition of batch.\n",
    "        \"\"\"\n",
    "        # ---- Setup initial values ----\n",
    "        batch_size = rewards[0].size(0)\n",
    "        R = torch.zeros(batch_size) # Reward accumulator\n",
    "        B = 0 # Baseline accumulator - used primarily for the average baseline case\n",
    "        loss = Variable(torch.Tensor([0]))\n",
    "        if self.iscuda:\n",
    "            loss = loss.cuda()\n",
    "            R = R.cuda()\n",
    "    \n",
    "        # Task-specific error computation\n",
    "        #for t in reversed(range(self.T)):\n",
    "        #    loss = loss + task_errs[t].sum()/batch_size\n",
    "    \n",
    "        # REINFORCE loss based on T-1 transitions.\n",
    "        # Note: This will automatically be ignored when self.T = 1\n",
    "        for t in reversed(range(self.T-1)):\n",
    "            R = R + rewards[t] # A one sample MC estimate of Q[t]\n",
    "            # PG loss\n",
    "            loss_term_1 = -(log_probs[t]*self.reward_scale).sum()/batch_size # Removed adv... Not sure what that does. \n",
    "            # Entropy loss, maximize entropy\n",
    "            loss_term_2 = -self.lambda_entropy*entropies[t].sum()/batch_size\n",
    "            loss = loss + loss_term_1 + loss_term_2\n",
    "    \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.policy.parameters(), 10)\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now try to create a reasonable algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce(env, policy_estimator, num_episodes=10, batch_size=10, gamma=0.99):    # Set up lists to hold results\n",
    "    total_rewards = []\n",
    "    batch_rewards = []\n",
    "    batch_actions = []\n",
    "    batch_states = []\n",
    "    batch_counter = 1\n",
    "    \n",
    "    # Define optimizer\n",
    "    optimizer = optim.Adam(policy_estimator.network.parameters(), lr=0.01)\n",
    "    \n",
    "    action_space = np.arange(env.action_space.n)\n",
    "    ep = 0\n",
    "    while ep < num_episodes:\n",
    "        s_0 = env.reset()\n",
    "        states = []\n",
    "        rewards = []\n",
    "        actions = []\n",
    "        done = False\n",
    "        while done == False:\n",
    "            # Get actions and convert to numpy array\n",
    "            action_probs = policy_estimator.predict(s_0).detach().numpy()\n",
    "            action = np.random.choice(action_space, p=action_probs)\n",
    "            s_1, r, done, _ = env.step(action)\n",
    "            states.append(s_0)\n",
    "            rewards.append(r)\n",
    "            actions.append(action)\n",
    "            s_0 = s_1\n",
    "            \n",
    "            # If done, batch data\n",
    "            if done:\n",
    "                batch_rewards.extend(discount_rewards(rewards, gamma))\n",
    "                batch_states.extend(states)\n",
    "                batch_actions.extend(actions)\n",
    "                batch_counter += 1\n",
    "                total_rewards.append(sum(rewards))\n",
    "                \n",
    "                # If batch is complete, update network\n",
    "                if batch_counter == batch_size:\n",
    "                    optimizer.zero_grad()\n",
    "                    state_tensor = torch.FloatTensor(batch_states)\n",
    "                    reward_tensor = torch.FloatTensor(batch_rewards)\n",
    "                    # Actions are used as indices, must be LongTensor\n",
    "                    action_tensor = torch.LongTensor(batch_actions)\n",
    "                    \n",
    "                    # Calculate loss\n",
    "                    logprob = torch.log(policy_estimator.predict(state_tensor))\n",
    "                    selected_logprobs = reward_tensor * torch.gather(logprob, 1, action_tensor).squeeze()\n",
    "                    loss = -selected_logprobs.mean()\n",
    "                    \n",
    "                    # Calculate gradients\n",
    "                    loss.backward()\n",
    "                    # Apply gradients\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    batch_rewards = []\n",
    "                    batch_actions = []\n",
    "                    batch_states = []\n",
    "                    batch_counter = 1\n",
    "                    \n",
    "                avg_rewards = np.mean(total_rewards[-100:])\n",
    "                # Print running average\n",
    "                print(\"\\rEp: {} Average of last 100:\" + \"{:.2f}\".format(ep + 1, avg_rewards), end=\"\")\n",
    "                ep += 1\n",
    "                \n",
    "    return total_rewards"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
