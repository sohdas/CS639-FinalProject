{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "\n",
    "sys.path.append(r'/home/declan/Documents/Projects/CS639-FinalProject/src/reconstruction/')\n",
    "from dataset import FaceDataset\n",
    "from policy import Policy\n",
    "from utils import StateObject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  0\n",
      "torch.Size([32, 256])\n",
      "tensor([[4.7288e-03, 5.2285e-03, 3.6618e-03,  ..., 3.4695e-03, 4.2741e-03,\n",
      "         3.5362e-03],\n",
      "        [4.7288e-03, 5.2285e-03, 3.6618e-03,  ..., 3.4695e-03, 4.2741e-03,\n",
      "         3.5362e-03],\n",
      "        [4.7288e-03, 5.2285e-03, 3.6618e-03,  ..., 3.4695e-03, 4.2741e-03,\n",
      "         3.5362e-03],\n",
      "        ...,\n",
      "        [4.7288e-03, 5.2285e-03, 3.6618e-03,  ..., 3.4695e-03, 4.2741e-03,\n",
      "         3.5362e-03],\n",
      "        [1.7960e-04, 3.0948e-03, 4.0607e-03,  ..., 9.3396e-05, 5.0793e-04,\n",
      "         3.2146e-03],\n",
      "        [1.7960e-04, 3.0948e-03, 4.0607e-03,  ..., 9.3396e-05, 5.0793e-04,\n",
      "         3.2146e-03]], grad_fn=<SoftmaxBackward>)\n",
      "Iteration  1\n",
      "torch.Size([32, 256])\n",
      "tensor([[3.8889e-03, 4.5614e-03, 3.8290e-03,  ..., 3.5281e-03, 3.8896e-03,\n",
      "         3.9960e-03],\n",
      "        [3.8889e-03, 4.5614e-03, 3.8290e-03,  ..., 3.5281e-03, 3.8896e-03,\n",
      "         3.9960e-03],\n",
      "        [3.8889e-03, 4.5614e-03, 3.8290e-03,  ..., 3.5281e-03, 3.8896e-03,\n",
      "         3.9960e-03],\n",
      "        ...,\n",
      "        [3.8889e-03, 4.5614e-03, 3.8290e-03,  ..., 3.5281e-03, 3.8896e-03,\n",
      "         3.9960e-03],\n",
      "        [3.3870e-03, 1.7331e-02, 1.0642e-03,  ..., 7.1690e-05, 1.0220e-03,\n",
      "         8.6045e-04],\n",
      "        [3.3870e-03, 1.7331e-02, 1.0642e-03,  ..., 7.1690e-05, 1.0220e-03,\n",
      "         8.6045e-04]], grad_fn=<SoftmaxBackward>)\n",
      "Iteration  2\n",
      "torch.Size([32, 256])\n",
      "tensor([[0.0037, 0.0049, 0.0036,  ..., 0.0030, 0.0041, 0.0038],\n",
      "        [0.0037, 0.0049, 0.0036,  ..., 0.0030, 0.0041, 0.0038],\n",
      "        [0.0037, 0.0049, 0.0036,  ..., 0.0030, 0.0041, 0.0038],\n",
      "        ...,\n",
      "        [0.0037, 0.0049, 0.0036,  ..., 0.0030, 0.0041, 0.0038],\n",
      "        [0.0077, 0.0080, 0.0041,  ..., 0.0010, 0.0007, 0.0022],\n",
      "        [0.0077, 0.0080, 0.0041,  ..., 0.0010, 0.0007, 0.0022]],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "Iteration  3\n",
      "torch.Size([32, 256])\n",
      "tensor([[0.0040, 0.0048, 0.0039,  ..., 0.0031, 0.0039, 0.0044],\n",
      "        [0.0040, 0.0048, 0.0039,  ..., 0.0031, 0.0039, 0.0044],\n",
      "        [0.0040, 0.0048, 0.0039,  ..., 0.0031, 0.0039, 0.0044],\n",
      "        ...,\n",
      "        [0.0040, 0.0048, 0.0039,  ..., 0.0031, 0.0039, 0.0044],\n",
      "        [0.0027, 0.0115, 0.0011,  ..., 0.0005, 0.0011, 0.0003],\n",
      "        [0.0027, 0.0115, 0.0011,  ..., 0.0005, 0.0011, 0.0003]],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "Iteration  4\n",
      "torch.Size([32, 256])\n",
      "tensor([[0.0037, 0.0045, 0.0040,  ..., 0.0029, 0.0038, 0.0043],\n",
      "        [0.0037, 0.0045, 0.0040,  ..., 0.0029, 0.0038, 0.0043],\n",
      "        [0.0037, 0.0045, 0.0040,  ..., 0.0029, 0.0038, 0.0043],\n",
      "        ...,\n",
      "        [0.0037, 0.0045, 0.0040,  ..., 0.0029, 0.0038, 0.0043],\n",
      "        [0.0062, 0.0238, 0.0007,  ..., 0.0012, 0.0018, 0.0004],\n",
      "        [0.0062, 0.0238, 0.0007,  ..., 0.0012, 0.0018, 0.0004]],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "Iteration  5\n",
      "torch.Size([32, 256])\n",
      "tensor([[0.0036, 0.0048, 0.0041,  ..., 0.0029, 0.0040, 0.0041],\n",
      "        [0.0036, 0.0048, 0.0041,  ..., 0.0029, 0.0040, 0.0041],\n",
      "        [0.0036, 0.0048, 0.0041,  ..., 0.0029, 0.0040, 0.0041],\n",
      "        ...,\n",
      "        [0.0036, 0.0048, 0.0041,  ..., 0.0029, 0.0040, 0.0041],\n",
      "        [0.0104, 0.0090, 0.0005,  ..., 0.0015, 0.0012, 0.0008],\n",
      "        [0.0104, 0.0090, 0.0005,  ..., 0.0015, 0.0012, 0.0008]],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "Iteration  6\n",
      "torch.Size([32, 256])\n",
      "tensor([[0.0041, 0.0047, 0.0041,  ..., 0.0032, 0.0038, 0.0040],\n",
      "        [0.0041, 0.0047, 0.0041,  ..., 0.0032, 0.0038, 0.0040],\n",
      "        [0.0041, 0.0047, 0.0041,  ..., 0.0032, 0.0038, 0.0040],\n",
      "        ...,\n",
      "        [0.0041, 0.0047, 0.0041,  ..., 0.0032, 0.0038, 0.0040],\n",
      "        [0.0015, 0.0118, 0.0004,  ..., 0.0003, 0.0019, 0.0010],\n",
      "        [0.0015, 0.0118, 0.0004,  ..., 0.0003, 0.0019, 0.0010]],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "Iteration  7\n",
      "torch.Size([32, 256])\n",
      "tensor([[0.0042, 0.0049, 0.0040,  ..., 0.0031, 0.0043, 0.0042],\n",
      "        [0.0042, 0.0049, 0.0040,  ..., 0.0031, 0.0043, 0.0042],\n",
      "        [0.0042, 0.0049, 0.0040,  ..., 0.0031, 0.0043, 0.0042],\n",
      "        ...,\n",
      "        [0.0042, 0.0049, 0.0040,  ..., 0.0031, 0.0043, 0.0042],\n",
      "        [0.0015, 0.0110, 0.0011,  ..., 0.0004, 0.0004, 0.0007],\n",
      "        [0.0015, 0.0110, 0.0011,  ..., 0.0004, 0.0004, 0.0007]],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "Iteration  8\n",
      "torch.Size([32, 256])\n",
      "tensor([[0.0040, 0.0042, 0.0038,  ..., 0.0031, 0.0041, 0.0038],\n",
      "        [0.0040, 0.0042, 0.0038,  ..., 0.0031, 0.0041, 0.0038],\n",
      "        [0.0040, 0.0042, 0.0038,  ..., 0.0031, 0.0041, 0.0038],\n",
      "        ...,\n",
      "        [0.0040, 0.0042, 0.0038,  ..., 0.0031, 0.0041, 0.0038],\n",
      "        [0.0030, 0.0774, 0.0017,  ..., 0.0008, 0.0007, 0.0023],\n",
      "        [0.0030, 0.0774, 0.0017,  ..., 0.0008, 0.0007, 0.0023]],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "Iteration  9\n",
      "torch.Size([32, 256])\n",
      "tensor([[0.0042, 0.0049, 0.0037,  ..., 0.0032, 0.0042, 0.0041],\n",
      "        [0.0042, 0.0049, 0.0037,  ..., 0.0032, 0.0042, 0.0041],\n",
      "        [0.0042, 0.0049, 0.0037,  ..., 0.0032, 0.0042, 0.0041],\n",
      "        ...,\n",
      "        [0.0042, 0.0049, 0.0037,  ..., 0.0032, 0.0042, 0.0041],\n",
      "        [0.0007, 0.0073, 0.0025,  ..., 0.0004, 0.0005, 0.0008],\n",
      "        [0.0007, 0.0073, 0.0025,  ..., 0.0004, 0.0005, 0.0008]],\n",
      "       grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.zeros([32, 3, 512, 512]) # Random input images. batch size=32.\n",
    "#action_probs = np.random.uniform(low=0, high=1, size=(32, 256)) # Random action probs.\n",
    "state_object = StateObject(tensor)   # Create the view object.\n",
    "#state_object.get_views(action_probs) # Get views corresponding to the max prob actions.\n",
    "\n",
    "im, pro = state_object.get_view()\n",
    "im, pro = torch.Tensor(im), torch.Tensor(pro)\n",
    "\n",
    "# ---- Policy forward pass ----\n",
    "policy_input = {'im': im, 'pro': pro}\n",
    "\n",
    "tensor = torch.zeros([32, 3, 512, 512])\n",
    "#model = Policy()\n",
    "#probs, hidden, decoded = model(policy_input)\n",
    "\n",
    "# Test whether the agent works.\n",
    "agent = Agent()\n",
    "log_probs, rec_errs, rewards, entropies, decoded, visited_idxes, decoded_all, _ = agent.gather_trajectory(state_object)\n",
    "\n",
    "#log_probs, rec_errs, rewards, entropies, decoded, values, visited_idxes, decoded_all, _ = agent.gather_trajectory(state, eval_opts=None, pano_maps=pano_maps, opts=opts)\n",
    "# Backward pass\n",
    "#agent.update_policy(rewards, log_probs, rec_errs, entropies, values) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \"\"\"\n",
    "    This agent implements the policy from Policy class and uses REINFORCE / Actor-Critic for policy improvement\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Agent, self).__init__()\n",
    "        self.policy = Policy()\n",
    "        self.iscuda = False\n",
    "\n",
    "\n",
    "    def gather_trajectory(self, state_object):\n",
    "        \"\"\"\n",
    "        gather_trajectory gets an observation, updates it's belief of the state, decodes the\n",
    "        panorama and takes the next action. This is done repeatedly for T time steps.\n",
    "        Note:\n",
    "        eval_opts are provided only during testing\n",
    "        \"\"\"\n",
    "\n",
    "        # Setup variables to store trajectory information.\n",
    "        self.T = 10\n",
    "        rewards = []\n",
    "        log_probs = []\n",
    "        rec_errs = []\n",
    "        entropies = []\n",
    "        self.avg_count = 0\n",
    "        hidden = None\n",
    "        visited_idxes = []\n",
    "        batch_size = state_object.batch_size\n",
    "        decoded_all = []\n",
    "        actions_taken = torch.zeros(batch_size, self.T-1)\n",
    "        action_probs = None\n",
    "\n",
    "        \n",
    "        for t in range(self.T):\n",
    "            #print('Iteration ', str(t))\n",
    "            # ---- Observe the panorama ----\n",
    "            # At first iteration, action probs==None, so state_object returns a fixed view at (0,0).\n",
    "            im, pro = state_object.get_view(action_probs)\n",
    "            im, pro = torch.Tensor(im), torch.Tensor(pro)\n",
    "            \n",
    "            # Keep track of visited locations\n",
    "            visited_idxes.append(state_object.index)\n",
    "\n",
    "            # ---- Policy forward pass ----\n",
    "            policy_input = {'im': im, 'pro': pro}\n",
    "            \n",
    "            # Update if using CUDA.\n",
    "            if self.iscuda:\n",
    "                for var in policy_input:\n",
    "                    policy_input[var] = policy_input[var].cuda()\n",
    "            \n",
    "            # Otherwise use the CPU.\n",
    "            else:\n",
    "                for var in policy_input:\n",
    "                    policy_input[var] = Variable(policy_input[var])\n",
    "\n",
    "            # Note: decoded and hidden correspond to the previous transition\n",
    "            # probs and value correspond to the new transition, where the value\n",
    "            # and action probabilities of the current state are estimated for PG update.\n",
    "            action_probs, hidden, decoded = self.policy.forward(policy_input, hidden)\n",
    "            #print(action_probs.shape)\n",
    "            #print(action_probs)\n",
    "\n",
    "            # Compute reconstruction loss (corresponding to the previous transition).\n",
    "            ###### NOTE: MAY BE APPROPRIATE TO USE A DIFFERENT LOSS FUNCTION HERE.\n",
    "            rec_err = F.mse_loss(decoded, state_object.images) #state_object.loss_fn(decoded, self.iscuda)\n",
    "            # \n",
    "\n",
    "            # Reconstruction reward is obtained only at the final step\n",
    "            # If there is only one step (T=1), then do not provide rewards\n",
    "            # Note: This reward corresponds to the previous action\n",
    "            if t < self.T-1 or t == 0:\n",
    "                reward = torch.zeros(batch_size)\n",
    "                if self.iscuda:\n",
    "                    reward = reward.cuda()\n",
    "            ##### I HAVE NO IDEA WHAT'S GOING ON HERE. #####\n",
    "            else:\n",
    "                reward = -rec_err.data # Disconnects reward from future updates\n",
    "                #self.R_avg = (self.R_avg * self.avg_count + reward.sum())/(self.avg_count + batch_size)\n",
    "                #self.avg_count += batch_size\n",
    "            if t > 0:\n",
    "                rewards[t-1] += reward\n",
    "\n",
    "            # There are self.T reconstruction errors as opposed to self.T-1 rewards\n",
    "            rec_errs.append(rec_err)\n",
    "\n",
    "            # ---- Sample action ----\n",
    "            # except for the last time step when only the selected view from previous step is used in aggregate.\n",
    "            if t < self.T - 1:\n",
    "                # Act based on the policy network.\n",
    "                #if eval_opts == None or eval_opts['greedy'] == False:\n",
    "                #    act = action_probs.multinomial(num_samples=1).data\n",
    "                #else:\n",
    "                \n",
    "                #### NOTE: NOT ENTIRELY SURE WHAT'S GOING ON HERE... ####\n",
    "                # This works only while evaluating, not while training\n",
    "                _, act = action_probs.max(dim=1)\n",
    "                act = act.data.view(-1, 1)\n",
    "                # Compute entropy\n",
    "                entropy = -(action_probs*((action_probs+1e-7).log())).sum(dim=1)\n",
    "                # Store log probabilities of selected actions (Advanced indexing)\n",
    "                log_prob = (action_probs[range(act.size(0)), act[:, 0]]+1e-7).log()\n",
    "\n",
    "                # This is the intrinsic reward corresponding to the current action\n",
    "                #### NOTE: NO IDEA WHAT THIS IS. PROBABLY WRONG.\n",
    "                rewards.append(reward) #\n",
    "                log_probs.append(log_prob)\n",
    "                entropies.append(entropy)\n",
    "\n",
    "        return log_probs, rec_errs, rewards, entropies, decoded, visited_idxes, decoded_all, actions_taken\n",
    "    \n",
    "    \n",
    "    def update_policy(self, rewards, log_probs, task_errs, entropies, values=None):\n",
    "        \"\"\"\n",
    "        This function will take the rewards, log probabilities and task-spencific errors from\n",
    "        the trajectory and perform the parameter updates for the policy using\n",
    "        REINFORCE / Actor-Critic.\n",
    "        INPUTS:\n",
    "            rewards: list of T-1 Tensors containing reward for each batch at each time step\n",
    "            log_probs: list of T-1 logprobs Variables of each transition of batch\n",
    "            task_errs: list of T error Variables for each transition of batch\n",
    "            entropies: list of T-1 entropy Variables for each transition of batch\n",
    "            values: list of T-1 predicted values Variables for each transition of batch\n",
    "        \"\"\"\n",
    "        # ---- Setup initial values ----\n",
    "        batch_size = task_errs[0].size(0)\n",
    "        R = torch.zeros(batch_size) # Reward accumulator\n",
    "        B = 0 # Baseline accumulator - used primarily for the average baseline case\n",
    "        loss = Variable(torch.Tensor([0]))\n",
    "        if self.iscuda:\n",
    "            loss = loss.cuda()\n",
    "            R = R.cuda()\n",
    "    \n",
    "        # Task-specific error computation\n",
    "        for t in reversed(range(self.T)):\n",
    "            loss = loss + task_errs[t].sum()/batch_size\n",
    "    \n",
    "        # REINFORCE loss based on T-1 transitions.\n",
    "        # Note: This will automatically be ignored when self.T = 1\n",
    "        for t in reversed(range(self.T-1)):\n",
    "            R = R + rewards[t] # A one sample MC estimate of Q[t]\n",
    "            if t == self.T-2:\n",
    "                B += self.R_avg\n",
    "            B += self.R_avg_expert * self.reward_scale_expert\n",
    "            adv = R - B\n",
    "            # PG loss\n",
    "            loss_term_1 = - (log_probs[t]*self.reward_scale*Variable(adv, requires_grad=False)).sum()/batch_size \n",
    "            # Entropy loss, maximize entropy\n",
    "            loss_term_2 = - self.lambda_entropy*entropies[t].sum()/batch_size\n",
    "            loss = loss + loss_term_1 + loss_term_2\n",
    "    \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm(self.policy.parameters(), 10)\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now try to create a reasonable algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce(env, policy_estimator, num_episodes=10, batch_size=10, gamma=0.99):    # Set up lists to hold results\n",
    "    total_rewards = []\n",
    "    batch_rewards = []\n",
    "    batch_actions = []\n",
    "    batch_states = []\n",
    "    batch_counter = 1\n",
    "    \n",
    "    # Define optimizer\n",
    "    optimizer = optim.Adam(policy_estimator.network.parameters(), lr=0.01)\n",
    "    \n",
    "    action_space = np.arange(env.action_space.n)\n",
    "    ep = 0\n",
    "    while ep < num_episodes:\n",
    "        s_0 = env.reset()\n",
    "        states = []\n",
    "        rewards = []\n",
    "        actions = []\n",
    "        done = False\n",
    "        while done == False:\n",
    "            # Get actions and convert to numpy array\n",
    "            action_probs = policy_estimator.predict(s_0).detach().numpy()\n",
    "            action = np.random.choice(action_space, p=action_probs)\n",
    "            s_1, r, done, _ = env.step(action)\n",
    "            states.append(s_0)\n",
    "            rewards.append(r)\n",
    "            actions.append(action)\n",
    "            s_0 = s_1\n",
    "            \n",
    "            # If done, batch data\n",
    "            if done:\n",
    "                batch_rewards.extend(discount_rewards(rewards, gamma))\n",
    "                batch_states.extend(states)\n",
    "                batch_actions.extend(actions)\n",
    "                batch_counter += 1\n",
    "                total_rewards.append(sum(rewards))\n",
    "                \n",
    "                # If batch is complete, update network\n",
    "                if batch_counter == batch_size:\n",
    "                    optimizer.zero_grad()\n",
    "                    state_tensor = torch.FloatTensor(batch_states)\n",
    "                    reward_tensor = torch.FloatTensor(batch_rewards)\n",
    "                    # Actions are used as indices, must be LongTensor\n",
    "                    action_tensor = torch.LongTensor(batch_actions)\n",
    "                    \n",
    "                    # Calculate loss\n",
    "                    logprob = torch.log(policy_estimator.predict(state_tensor))\n",
    "                    selected_logprobs = reward_tensor * torch.gather(logprob, 1, action_tensor).squeeze()\n",
    "                    loss = -selected_logprobs.mean()\n",
    "                    \n",
    "                    # Calculate gradients\n",
    "                    loss.backward()\n",
    "                    # Apply gradients\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    batch_rewards = []\n",
    "                    batch_actions = []\n",
    "                    batch_states = []\n",
    "                    batch_counter = 1\n",
    "                    \n",
    "                avg_rewards = np.mean(total_rewards[-100:])\n",
    "                # Print running average\n",
    "                print(\"\\rEp: {} Average of last 100:\" + \"{:.2f}\".format(ep + 1, avg_rewards), end=\"\")\n",
    "                ep += 1\n",
    "                \n",
    "    return total_rewards"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
