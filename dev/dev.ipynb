{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import to_tensor, to_pil_image\n",
    "\n",
    "sys.path.append(r'/home/declan/Documents/Projects/CS639-FinalProject/src/reconstruction/')\n",
    "from dataset import FaceDataset\n",
    "from policy import Policy\n",
    "from utils import StateObject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \"\"\"\n",
    "    This agent implements the policy from Policy class and uses REINFORCE / Actor-Critic for policy improvement\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Agent, self).__init__()\n",
    "        self.policy = Policy().to(device)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=5e-4)\n",
    "        self.iscuda = True\n",
    "        \n",
    "        # Maybe not necessary.\n",
    "        self.R_avg = 0\n",
    "        self.avg_count = 0\n",
    "        self.reward_scale = 1\n",
    "        self.lambda_entropy = .1\n",
    "        self.selection_type = 'greedy'\n",
    "\n",
    "\n",
    "    def gather_trajectory(self, state_object):\n",
    "        \"\"\"\n",
    "        gather_trajectory gets an observation, updates it's belief of the state, decodes the\n",
    "        panorama and takes the next action. This is done repeatedly for T time steps.\n",
    "        Note:\n",
    "        eval_opts are provided only during testing\n",
    "        \"\"\"\n",
    "\n",
    "        # Setup variables to store trajectory information.\n",
    "        self.T = 10\n",
    "        rewards = []\n",
    "        log_probs = []\n",
    "        rec_errs = []\n",
    "        entropies = []\n",
    "        self.avg_count = 0\n",
    "        hidden = None\n",
    "        visited_idxes = []\n",
    "        batch_size = state_object.batch_size\n",
    "        decoded_all = []\n",
    "        actions_taken = torch.zeros(batch_size, self.T-1)\n",
    "        action_probs = None\n",
    "        \n",
    "        for t in range(self.T):\n",
    "            #print('Iteration ', str(t))\n",
    "            # ---- Observe the panorama ----\n",
    "            # At first iteration, action probs==None, so state_object returns a fixed view at (0,0).\n",
    "            im, pro = state_object.get_view(action_probs)\n",
    "            im, pro = torch.Tensor(im), torch.Tensor(pro)\n",
    "            \n",
    "            # Keep track of visited locations\n",
    "            visited_idxes.append(state_object.index)\n",
    "\n",
    "            # ---- Policy forward pass ----\n",
    "            policy_input = {'im': im, 'pro': pro}\n",
    "            \n",
    "            # Update if using CUDA.\n",
    "            if self.iscuda:\n",
    "                for var in policy_input:\n",
    "                    policy_input[var] = policy_input[var].cuda()\n",
    "            \n",
    "            # Otherwise use the CPU.\n",
    "            #else:\n",
    "            for var in policy_input:\n",
    "                policy_input[var] = Variable(policy_input[var])\n",
    "\n",
    "            # Note: decoded and hidden correspond to the previous transition\n",
    "            # probs and value correspond to the new transition, where the value\n",
    "            # and action probabilities of the current state are estimated for PG update.\n",
    "            action_probs, hidden, decoded = self.policy.forward(policy_input, hidden)\n",
    "            decoded_all.append(decoded)\n",
    "\n",
    "            # Compute reconstruction loss (corresponding to the previous transition).\n",
    "            ###### NOTE: MAY BE APPROPRIATE TO USE A DIFFERENT LOSS FUNCTION HERE.\n",
    "            rec_err = F.mse_loss(decoded, state_object.images.to(device))\n",
    "            self.optimizer.zero_grad()\n",
    "            nn.utils.clip_grad_norm_(self.policy.parameters(), 10)\n",
    "            rec_err.backward(retain_graph=True)\n",
    "            self.optimizer.step()\n",
    "            \n",
    "\n",
    "            # Reconstruction reward is obtained only at the final step\n",
    "            # If there is only one step (T=1), then do not provide rewards\n",
    "            # Note: This reward corresponds to the previous action\n",
    "            if t == 0:\n",
    "                reward = torch.zeros(batch_size)\n",
    "                if self.iscuda:\n",
    "                    reward = reward.cuda()\n",
    "            ##### I HAVE NO IDEA WHAT'S GOING ON HERE. #####\n",
    "            else:\n",
    "                reward = -rec_err.data # Disconnects reward from future updates\n",
    "                self.R_avg = (self.R_avg * self.avg_count + reward.sum())/(self.avg_count + batch_size)\n",
    "                self.avg_count += batch_size\n",
    "            if t > 0:\n",
    "                rewards[t-1] += reward\n",
    "                \n",
    "            # There are self.T reconstruction errors as opposed to self.T-1 rewards\n",
    "            rec_errs.append(rec_err)\n",
    "\n",
    "            # Sample action except for the last time step when only the selected view from previous step is used in aggregate.\n",
    "            if t < self.T - 1:\n",
    "                # Act based on the policy network\n",
    "                if self.selection_type != 'greedy':\n",
    "                    act = action_probs.multinomial(num_samples=1).data\n",
    "                else:\n",
    "                    # This works only while evaluating, not while training\n",
    "                    _, act = action_probs.max(dim=1)\n",
    "                    act = act.data.view(-1, 1)\n",
    "                \n",
    "                # Compute entropy\n",
    "                entropy = -(action_probs*((action_probs+1e-7).log())).sum(dim=1)\n",
    "                # Store log probabilities of selected actions (Advanced indexing).\n",
    "                log_prob = (action_probs[range(act.size(0)), act[:, 0]]+1e-7).log() # Save log_probs of choosing selected actions.\n",
    "\n",
    "                # This is the intrinsic reward corresponding to the current action\n",
    "                #### NOTE: COULD BE WRONG.\n",
    "                rewards.append(reward)\n",
    "                log_probs.append(log_prob)\n",
    "                entropies.append(entropy)\n",
    "\n",
    "        return log_probs, rec_errs, rewards, entropies, decoded, visited_idxes, decoded_all, actions_taken\n",
    "    \n",
    "    \n",
    "    def update_policy(self, rewards, log_probs, entropies, task_errs=None, values=None):\n",
    "        \"\"\"\n",
    "        This function will take the rewards, log probabilities and task-specific errors from\n",
    "        the trajectory and perform the parameter updates for the policy using REINFORCE.\n",
    "        INPUTS:\n",
    "            rewards: list of T-1 Tensors containing reward for each batch at each time step.\n",
    "            log_probs: list of T-1 logprobs Variables of each transition of batch.\n",
    "            task_errs: list of T error Variables for each transition of batch.\n",
    "            entropies: list of T-1 entropy Variables for each transition of batch.\n",
    "            values: list of T-1 predicted values Variables for each transition of batch.\n",
    "        \"\"\"\n",
    "        # ---- Setup initial values ----\n",
    "        batch_size = rewards[0].size(0)\n",
    "        R = torch.zeros(batch_size) # Reward accumulator\n",
    "        B = 0 # Baseline accumulator - used primarily for the average baseline case\n",
    "        loss = Variable(torch.Tensor([0]))\n",
    "        if self.iscuda:\n",
    "            loss = loss.cuda()\n",
    "            R = R.cuda()\n",
    "    \n",
    "        # Task-specific error computation\n",
    "        #for t in reversed(range(self.T)):\n",
    "        #    loss = loss + task_errs[t].sum()/batch_size\n",
    "    \n",
    "        # REINFORCE loss based on T-1 transitions.\n",
    "        # Note: This will automatically be ignored when self.T = 1\n",
    "        for t in reversed(range(self.T-1)):\n",
    "            R = R + rewards[t] # A one sample MC estimate of Q[t]\n",
    "            # PG loss\n",
    "            loss_term_1 = -(log_probs[t]*self.reward_scale).sum()/batch_size # Removed adv... Not sure what that does. \n",
    "            # Entropy loss, maximize entropy\n",
    "            loss_term_2 = -self.lambda_entropy*entropies[t].sum()/batch_size\n",
    "            loss = loss + loss_term_1 #+ loss_term_2\n",
    "    \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.policy.parameters(), 10)\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need to figure out what's causing the memory leak.\n",
    "\n",
    "#### Some things it probably isn't after checking:\n",
    "\n",
    "<b>o</b> The decoded_all list doesn't seem to be a problem.\n",
    "\n",
    "<b>o</b> The retain_grad in the gather_trajectory method also doesn't seem to be a problem.\n",
    "\n",
    "<b>o</b> Probably not in StateObject. Memory leak still there even after only creating state obj for first batch_idx.\n",
    "\n",
    "<b>o</b> Probably not in update_policy. Leak still present after removing.\n",
    "\n",
    "#### <b>o</b> Probably is in the gather_trajectory method.\n",
    "\n",
    "<b>o</b> Probably not in the forward pass of Policy network.\n",
    "\n",
    "<b>o</b> Probably not in MSE computation.\n",
    "\n",
    "<b>o</b> Probably not in list tracking rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BI 0 Loss: 0.0995\n",
      "BI 1 Loss: 0.0915\n",
      "BI 2 Loss: 0.0951\n",
      "BI 3 Loss: 0.0837\n",
      "BI 4 Loss: 0.0723\n",
      "BI 5 Loss: 0.0833\n",
      "BI 6 Loss: 0.0675\n",
      "BI 7 Loss: 0.0823\n",
      "BI 8 Loss: 0.0753\n",
      "BI 9 Loss: 0.0614\n",
      "BI 10 Loss: 0.0648\n",
      "BI 11 Loss: 0.0701\n",
      "BI 12 Loss: 0.0774\n",
      "BI 13 Loss: 0.0669\n",
      "BI 14 Loss: 0.0630\n",
      "BI 15 Loss: 0.0681\n",
      "BI 16 Loss: 0.0643\n",
      "BI 17 Loss: 0.0753\n",
      "BI 18 Loss: 0.0649\n",
      "BI 19 Loss: 0.0689\n",
      "BI 20 Loss: 0.0627\n",
      "BI 21 Loss: 0.0592\n",
      "BI 22 Loss: 0.0623\n",
      "BI 23 Loss: 0.0693\n",
      "BI 24 Loss: 0.0774\n",
      "BI 25 Loss: 0.0722\n",
      "BI 26 Loss: 0.0634\n",
      "BI 27 Loss: 0.0674\n",
      "BI 28 Loss: 0.0655\n",
      "BI 29 Loss: 0.0647\n",
      "BI 30 Loss: 0.0626\n",
      "BI 31 Loss: 0.0677\n",
      "BI 32 Loss: 0.0684\n",
      "BI 33 Loss: 0.0645\n",
      "BI 34 Loss: 0.0594\n",
      "BI 35 Loss: 0.0496\n",
      "BI 36 Loss: 0.0525\n",
      "BI 37 Loss: 0.0612\n",
      "BI 38 Loss: 0.0633\n",
      "BI 39 Loss: 0.0594\n",
      "BI 40 Loss: 0.0542\n",
      "BI 41 Loss: 0.0560\n",
      "BI 42 Loss: 0.0586\n",
      "BI 43 Loss: 0.0627\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' #'cpu'\n",
    "torch.backends.cudnn.benchmark = True\n",
    "params = {'batch_size': 32,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 8}\n",
    "\n",
    "# Set up the Datasets and the DataLoaders.\n",
    "transforms_ = transforms.Compose([transforms.Resize(size=(512, 512)) , transforms.ToTensor()])\n",
    "dataset = FaceDataset(transforms=transforms_)\n",
    "data_loader = DataLoader(dataset, **params)\n",
    "\n",
    "# Try using a DataFolder instead.\n",
    "#TRAIN_DATA_PATH = '/home/declan/Data/Faces/'\n",
    "#train_data = torchvision.datasets.ImageFolder(root=TRAIN_DATA_PATH, transform=transforms_)\n",
    "#data_loader = DataLoader(train_data, **params)\n",
    "\n",
    "\n",
    "# Set up the model and the optimizer.\n",
    "agent = Agent()\n",
    "\n",
    "# Begin training model.\n",
    "n_epochs = 100\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_total_loss = 0.0\n",
    "    for batch_idx, data in enumerate(data_loader):\n",
    "\n",
    "        images, _ = data\n",
    "        state_object = StateObject(images)\n",
    "        \n",
    "        ### THIS IS A HACKY SOLUTION FOR NOW ### was running into issues with indexing on the final batch where batch_size<32\n",
    "        if images.shape[0] != 32:\n",
    "            continue\n",
    "        \n",
    "        log_probs, rec_errs, rewards, entropies, decoded, visited_idxes, decoded_all, _ = agent.gather_trajectory(state_object)\n",
    "        agent.update_policy(rewards, log_probs, entropies)\n",
    "        \n",
    "        # Print useful info.\n",
    "        rec_errs = [err.cpu().detach().numpy() for err in rec_errs] # Always detach variables from the graph :()\n",
    "        loss = sum(rec_errs)/len(rec_errs)\n",
    "        epoch_total_loss = epoch_total_loss + loss\n",
    "        print('BI {} Loss: {:.4f}'.format(batch_idx, loss))\n",
    "    print('Epoch {} Loss: {:.4f}'.format(epoch, epoch_total_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_img = 20\n",
    "img = decoded_all[0][i_img].detach().cpu()\n",
    "to_pil_image(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_view = 9\n",
    "img = decoded_all[i_view][i_img].detach().cpu()\n",
    "to_pil_image(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = images[i_img].detach()\n",
    "to_pil_image(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now try to create a reasonable algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce(env, policy_estimator, num_episodes=10, batch_size=10, gamma=0.99):    # Set up lists to hold results\n",
    "    total_rewards = []\n",
    "    batch_rewards = []\n",
    "    batch_actions = []\n",
    "    batch_states = []\n",
    "    batch_counter = 1\n",
    "    \n",
    "    # Define optimizer\n",
    "    optimizer = optim.Adam(policy_estimator.network.parameters(), lr=0.01)\n",
    "    \n",
    "    action_space = np.arange(env.action_space.n)\n",
    "    ep = 0\n",
    "    while ep < num_episodes:\n",
    "        s_0 = env.reset()\n",
    "        states = []\n",
    "        rewards = []\n",
    "        actions = []\n",
    "        done = False\n",
    "        while done == False:\n",
    "            # Get actions and convert to numpy array\n",
    "            action_probs = policy_estimator.predict(s_0).detach().numpy()\n",
    "            action = np.random.choice(action_space, p=action_probs)\n",
    "            s_1, r, done, _ = env.step(action)\n",
    "            states.append(s_0)\n",
    "            rewards.append(r)\n",
    "            actions.append(action)\n",
    "            s_0 = s_1\n",
    "            \n",
    "            # If done, batch data\n",
    "            if done:\n",
    "                batch_rewards.extend(discount_rewards(rewards, gamma))\n",
    "                batch_states.extend(states)\n",
    "                batch_actions.extend(actions)\n",
    "                batch_counter += 1\n",
    "                total_rewards.append(sum(rewards))\n",
    "                \n",
    "                # If batch is complete, update network\n",
    "                if batch_counter == batch_size:\n",
    "                    optimizer.zero_grad()\n",
    "                    state_tensor = torch.FloatTensor(batch_states)\n",
    "                    reward_tensor = torch.FloatTensor(batch_rewards)\n",
    "                    # Actions are used as indices, must be LongTensor\n",
    "                    action_tensor = torch.LongTensor(batch_actions)\n",
    "                    \n",
    "                    # Calculate loss\n",
    "                    logprob = torch.log(policy_estimator.predict(state_tensor))\n",
    "                    selected_logprobs = reward_tensor * torch.gather(logprob, 1, action_tensor).squeeze()\n",
    "                    loss = -selected_logprobs.mean()\n",
    "                    \n",
    "                    # Calculate gradients\n",
    "                    loss.backward()\n",
    "                    # Apply gradients\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    batch_rewards = []\n",
    "                    batch_actions = []\n",
    "                    batch_states = []\n",
    "                    batch_counter = 1\n",
    "                    \n",
    "                avg_rewards = np.mean(total_rewards[-100:])\n",
    "                # Print running average\n",
    "                print(\"\\rEp: {} Average of last 100:\" + \"{:.2f}\".format(ep + 1, avg_rewards), end=\"\")\n",
    "                ep += 1\n",
    "                \n",
    "    return total_rewards"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
