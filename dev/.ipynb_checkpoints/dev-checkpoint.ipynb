{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms.functional import to_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_file = 'CelebAMask-HQ-attribute-anno.txt'\n",
    "datadir = '/home/declan/Data/Faces/'\n",
    "img_dir = os.path.join(datadir, 'CelebA-HQ-img')\n",
    "attr_dir = os.path.join(datadir, 'CelebAMask-HQ-attribute-anno.txt')\n",
    "attributes = pd.read_csv(attr_dir, delimiter=' ')\n",
    "good_data = attributes[['Bald', 'Smiling', 'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Lipstick', 'Wearing_Necktie', 'Wearing_Necklace', 'Gray_Hair', 'Eyeglasses']]\n",
    "good_data = good_data.sample(frac=.05) ##### CHANGE THIS LATER ######\n",
    "good_data = good_data.clip(lower=0) # Set -1 to 0.\n",
    "good_data = good_data[(good_data.T != 0).any()] # drop any rows with only zeros.\n",
    "img_names = good_data.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0.jpg', '1.jpg', '2.jpg', ..., '29997.jpg', '29998.jpg',\n",
       "       '29999.jpg'], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Generates a basic Rumelhart dataset.\n",
    "'''\n",
    "class FaceDataset(Dataset):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Load the features.\n",
    "        attribute_file = 'CelebAMask-HQ-attribute-anno.txt'\n",
    "        datadir = '/home/declan/Data/Faces/'\n",
    "        img_dir = os.path.join(datadir, 'CelebA-HQ-img')\n",
    "        attr_dir = os.path.join(datadir, 'CelebAMask-HQ-attribute-anno.txt')\n",
    "        attributes = pd.read_csv(attr_dir, delimiter=' ')\n",
    "        good_data = attributes[['Bald', 'Smiling', 'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Lipstick', 'Wearing_Necktie', 'Wearing_Necklace', 'Gray_Hair', 'Eyeglasses']]\n",
    "        good_data = good_data.sample(frac=.01) ##### CHANGE THIS LATER ######\n",
    "        good_data = good_data.clip(lower=0) # Set -1 to 0.\n",
    "        good_data = good_data[(good_data.T != 0).any()] # drop any rows with only zeros.\n",
    "        img_names = good_data.index.values\n",
    "        self.images = [to_tensor(Image.open(os.path.join(datadir, 'CelebA-HQ-img', img))) for img in img_names]\n",
    "        self.features = good_data.values\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.images[index], self.features[index]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters to keep track of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a training function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(opts):\n",
    "\n",
    "    # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the policy network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "            \n",
    "        # Network settings\n",
    "        self.rnn_hidden_size = 256\n",
    "        self.iscuda = False\n",
    "        # The input size for location embedding / proprioception stack\n",
    "        self.input_size_loc = 2 # Relative camera position\n",
    "        self.input_size_act = 256\n",
    "\n",
    "        # (1) Sense image: Takes in BxCx32x32 image input and converts it to Bx256 matrix.\n",
    "        self.sense_im = nn.Sequential(self._conv(3, 16),      # Bx16x16x16\n",
    "                                      self._conv(16, 32),     # Bx32x8x8\n",
    "                                      self._conv(32, 64),     # Bx64x4x4\n",
    "                                      View((-1, 1024)),       # Bx1024\n",
    "                                      self._linear(1024, 256) # Bx256\n",
    "                                     )\n",
    "\n",
    "        # (2) Sense proprioception stack: Converts proprioception inputs to 16-D vector.\n",
    "        self.sense_pro = self._linear(self.input_size_loc, 16)\n",
    "\n",
    "        # (3) Fuse: Fusing the outputs of (1) and (2) to give 256-D vector per image\n",
    "        # May be appropriate to add activation function later or change to self._linear.\n",
    "        self.fuse = nn.Sequential(self._linear(272, 256), # Bx256\n",
    "                                         nn.Linear(256, 256),    # Bx256\n",
    "                                         nn.BatchNorm1d(256)\n",
    "                                        )\n",
    "\n",
    "        # (4) Aggregator: View aggregating LSTM\n",
    "        self.aggregate = nn.LSTM(input_size=256, hidden_size=self.rnn_hidden_size, num_layers=1)\n",
    "\n",
    "        # (5) Act module: Takes in aggregator hidden state to produce probability distribution over actions \n",
    "        self.act = nn.Sequential(self._linear(self.input_size_act, 128),\n",
    "                                 self._linear(128, 128),\n",
    "                                 nn.Linear(128, 256)    # because (512/32)**2=256\n",
    "                                )\n",
    "        \n",
    "        # (6) Decode module: Given the current representation of the image, reconstruct the full view.\n",
    "        self.decode = nn.Sequential(self._linear(256, 1024), # Bx1024\n",
    "                                    View((-1, 64, 4, 4)),    # Bx64x4x4\n",
    "                                    self._deconv(64, 64),    # Bx64x8x8\n",
    "                                    self._deconv(64, 32),    # Bx32x16x16\n",
    "                                    self._deconv(32, 32),    # Bx32x32x32\n",
    "                                    self._deconv(32, 16),    # Bx16x64x64\n",
    "                                    self._deconv(16, 16),    # Bx16x128x128\n",
    "                                    self._deconv(16, 8),     # Bx8x256x256\n",
    "                                    self._deconv(8, 3)       # Bx3x512x512\n",
    "                                   )\n",
    " \n",
    "            \n",
    "    def forward(self, x, hidden=None):\n",
    "        \n",
    "        # \"Sense\" the image.\n",
    "        x1 = self.sense_im(x['im']) #.squeeze()\n",
    "        x2 = self.sense_pro(x['pro'])\n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Set up the recurrent hidden layers.\n",
    "        # NOTE: confused about why there are two hidden layers here. Maybe that's just how RNN's work in PyTorch?\n",
    "        if hidden is None:\n",
    "            hidden = [Variable(torch.zeros(1, batch_size, self.rnn_hidden_size)), # hidden state: (num_layers, batch_size, hidden size)\n",
    "                      Variable(torch.zeros(1, batch_size, self.rnn_hidden_size))] # cell state  :(num_layers, batch_size, hidden size)\n",
    "            if self.iscuda:\n",
    "                hidden[0] = hidden[0].cuda()\n",
    "                hidden[1] = hidden[1].cuda()\n",
    "            \n",
    "        # Fuse the proprioceptive representation and the view representation.\n",
    "        x = self.fuse(x) \n",
    "\n",
    "        # Update the belief state about the image.\n",
    "        # Note: input to aggregate lstm has to be (seq_length x batch_size x input_dims)\n",
    "        # Since we are feeding in the inputs one by one, it is 1 x batch_size x 256\n",
    "        x, hidden = self.aggregate(x.view(1, *x.size()), hidden)\n",
    "        \n",
    "        # Define input to the action and decoding layers.\n",
    "        act_input = hidden[0].view(batch_size, -1)\n",
    "\n",
    "        # Predict the probability of all actions.\n",
    "        probs = F.softmax(self.act(act_input), dim=1)\n",
    "            \n",
    "        # Decode the whole image using the decoder.\n",
    "        decoded = self.decode(act_input)\n",
    "\n",
    "        return probs, hidden, decoded\n",
    "    \n",
    "    def _linear(self, in_size, out_size):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(in_size, out_size),\n",
    "            nn.BatchNorm1d(out_size),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "            \n",
    "    def _conv(self, in_size, out_size):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_size, out_size, kernel_size=5, stride=1, padding=2),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def _deconv(self, in_size, out_size):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_size, out_size, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "class View(nn.Module):\n",
    "    def __init__(self, shape):\n",
    "        super(View, self).__init__()\n",
    "        self.shape = shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(*self.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test that the Policy network works (or at least doesn't throw errors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 256])\n",
      "torch.Size([32, 16])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.zeros([32, 3, 512, 512]) # Random input images. batch size=32.\n",
    "#action_probs = np.random.uniform(low=0, high=1, size=(32, 256)) # Random action probs.\n",
    "state_object = StateObject(tensor)   # Create the view object.\n",
    "#state_object.get_views(action_probs) # Get views corresponding to the max prob actions.\n",
    "\n",
    "im, pro = state_object.get_view()\n",
    "im, pro = torch.Tensor(im), torch.Tensor(pro)\n",
    "\n",
    "# ---- Policy forward pass ----\n",
    "policy_input = {'im': im, 'pro': pro}\n",
    "\n",
    "tensor = torch.zeros([32, 3, 512, 512])\n",
    "model = Policy()\n",
    "probs, hidden, decoded = model(policy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 256])"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 2])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pro.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 32, 32])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing out different view encoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 16, 16, 16])\n",
      "torch.Size([64, 32, 8, 8])\n",
      "torch.Size([64, 64, 4, 4])\n",
      "torch.Size([64, 256])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.zeros([64, 3, 32, 32])\n",
    "conv1 = nn.Conv2d(3, 16, 5, stride=1, padding=2)\n",
    "maxpool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "conv2 = nn.Conv2d(16, 32, 5, stride=1, padding=2)\n",
    "maxpool2 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "conv3 = nn.Conv2d(32, 64, 5, stride=1, padding=2)\n",
    "maxpool3 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "linear = nn.Linear(1024, 256)\n",
    "x = conv1(tensor)\n",
    "#print(x.shape)\n",
    "x = maxpool1(x)\n",
    "print(x.shape)\n",
    "x = conv2(x)\n",
    "#print(x.shape)\n",
    "x = maxpool2(x)\n",
    "print(x.shape)\n",
    "x = conv3(x)\n",
    "#print(x.shape)\n",
    "x = maxpool3(x)\n",
    "print(x.shape)\n",
    "x = x.view(64, -1)\n",
    "x = linear(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing out different view decoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 8, 512, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 512, 512])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.zeros([64, 256])\n",
    "linear = nn.Linear(256, 1024)\n",
    "convT1 = nn.ConvTranspose2d(64, 64, 4, stride=2, padding=1)\n",
    "convT2 = nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1)\n",
    "convT3 = nn.ConvTranspose2d(32, 32, 4, stride=2, padding=1)\n",
    "convT4 = nn.ConvTranspose2d(32, 16, 4, stride=2, padding=1)\n",
    "convT5 = nn.ConvTranspose2d(16, 16, 4, stride=2, padding=1)\n",
    "convT6 = nn.ConvTranspose2d(16, 8, 4, stride=2, padding=1)\n",
    "convT7 = nn.ConvTranspose2d(8, 8, 4, stride=2, padding=1)\n",
    "conv8 = nn.ConvTranspose2d(8, 3, 3, stride=1, padding=1)\n",
    "\n",
    "x = linear(tensor)\n",
    "x = x.view(-1, 64, 4, 4)\n",
    "x = convT1(x)\n",
    "x = convT2(x)\n",
    "x = convT3(x)\n",
    "x = convT4(x)\n",
    "x = convT5(x)\n",
    "x = convT6(x)\n",
    "x = convT7(x)\n",
    "print(x.shape)\n",
    "x = conv8(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BaseAgent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-112-1b9872135b7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseAgent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \"\"\"\n\u001b[1;32m      3\u001b[0m     \u001b[0mThis\u001b[0m \u001b[0magent\u001b[0m \u001b[0mimplements\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpolicy\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPolicy\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0muses\u001b[0m \u001b[0mREINFORCE\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mActor\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mCritic\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpolicy\u001b[0m \u001b[0mimprovement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \"\"\"\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BaseAgent' is not defined"
     ]
    }
   ],
   "source": [
    "class Agent(BaseAgent):\n",
    "    \"\"\"\n",
    "    This agent implements the policy from Policy class and uses REINFORCE / Actor-Critic for policy improvement\n",
    "    \"\"\"\n",
    "    def __init__(self, opts, mode='train'):\n",
    "        super(Agent, self).__init__(opts, mode=mode)\n",
    "\n",
    "\n",
    "    def gather_trajectory(self, state_object, ):\n",
    "        \"\"\"\n",
    "        gather_trajectory gets an observation, updates it's belief of the state, decodes the\n",
    "        panorama and takes the next action. This is done repeatedly for T time steps.\n",
    "        Note:\n",
    "        eval_opts are provided only during testing\n",
    "        \"\"\"\n",
    "\n",
    "        # Setup variables to store trajectory information.\n",
    "        rewards = []\n",
    "        log_probs = []\n",
    "        rec_errs = []\n",
    "        entropies = []\n",
    "        hidden = None\n",
    "        visited_idxes = []\n",
    "        batch_size = state_object.batch_size\n",
    "        decoded_all = []\n",
    "        actions_taken = torch.zeros(batch_size, self.T-1)\n",
    "\n",
    "        \n",
    "        for t in range(self.T):\n",
    "            # ---- Observe the panorama ----\n",
    "            im, pro = state_object.get_view()\n",
    "            im, pro = torch.Tensor(im), torch.Tensor(pro)\n",
    "            \n",
    "            # Keep track of visited locations\n",
    "            visited_idxes.append(state_object.idx)\n",
    "\n",
    "            # ---- Policy forward pass ----\n",
    "            policy_input = {'im': im, 'pro': pro}\n",
    "            \n",
    "            # Update if using CUDA.\n",
    "            if self.iscuda:\n",
    "                for var in policy_input:\n",
    "                    policy_input[var] = policy_input[var].cuda()\n",
    "            \n",
    "            # Otherwise use the CPU.\n",
    "            else:\n",
    "                for var in policy_input:\n",
    "                    policy_input[var] = Variable(policy_input[var])\n",
    "\n",
    "            # Note: decoded and hidden correspond to the previous transition\n",
    "            # probs and value correspond to the new transition, where the value\n",
    "            # and action probabilities of the current state are estimated for PG update.\n",
    "            probs, hidden, decoded = self.policy.forward(policy_input, hidden)\n",
    "\n",
    "            # Compute reconstruction loss (corresponding to the previous transition).\n",
    "            ###### NOTE: MAY BE APPROPRIATE TO USE A DIFFERENT LOSS FUNCTION HERE.\n",
    "            rec_err = F.mse_loss(decoded, state_object.images) #state_object.loss_fn(decoded, self.iscuda)\n",
    "            # \n",
    "\n",
    "            # Reconstruction reward is obtained only at the final step\n",
    "            # If there is only one step (T=1), then do not provide rewards\n",
    "            # Note: This reward corresponds to the previous action\n",
    "            if t < self.T-1 or t == 0:\n",
    "                reward = torch.zeros(batch_size)\n",
    "                if self.iscuda:\n",
    "                    reward = reward.cuda()\n",
    "            else:\n",
    "                reward = -rec_err.data # Disconnects reward from future updates\n",
    "                self.R_avg = (self.R_avg * self.avg_count + reward.sum())/(self.avg_count + batch_size)\n",
    "                self.avg_count += batch_size\n",
    "            if t > 0:\n",
    "                rewards[t-1] += reward\n",
    "\n",
    "            # There are self.T reconstruction errors as opposed to self.T-1 rewards\n",
    "            rec_errs.append(rec_err)\n",
    "\n",
    "            # ---- Sample action ----\n",
    "            # except for the last time step when only the selected view from previous step is used in aggregate.\n",
    "            if t < self.T - 1:\n",
    "                # Act based on the policy network.\n",
    "                if eval_opts == None or eval_opts['greedy'] == False:\n",
    "                    act = probs.multinomial(num_samples=1).data\n",
    "                else:\n",
    "                    # This works only while evaluating, not while training\n",
    "                    _, act = probs.max(dim=1)\n",
    "                    act = act.data.view(-1, 1)\n",
    "                # Compute entropy\n",
    "                entropy = -(probs*((probs+1e-7).log())).sum(dim=1)\n",
    "                # Store log probabilities of selected actions (Advanced indexing)\n",
    "                log_prob = (probs[range(act.size(0)), act[:, 0]]+1e-7).log()\n",
    "\n",
    "                # This is the intrinsic reward corresponding to the current action\n",
    "                rewards.append(reward_expert*self.reward_scale_expert)\n",
    "                log_probs.append(log_prob)\n",
    "                entropies.append(entropy)\n",
    "\n",
    "        return log_probs, rec_errs, rewards, entropies, decoded, values, visited_idxes, decoded_all, actions_taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy(self, rewards, log_probs, task_errs, entropies, values=None):\n",
    "    \"\"\"\n",
    "    This function will take the rewards, log probabilities and task-spencific errors from\n",
    "    the trajectory and perform the parameter updates for the policy using\n",
    "    REINFORCE / Actor-Critic.\n",
    "    INPUTS:\n",
    "        rewards: list of T-1 Tensors containing reward for each batch at each time step\n",
    "        log_probs: list of T-1 logprobs Variables of each transition of batch\n",
    "        task_errs: list of T error Variables for each transition of batch\n",
    "        entropies: list of T-1 entropy Variables for each transition of batch\n",
    "        values: list of T-1 predicted values Variables for each transition of batch\n",
    "    \"\"\"\n",
    "    # ---- Setup initial values ----\n",
    "    batch_size = task_errs[0].size(0)\n",
    "    R = torch.zeros(batch_size) # Reward accumulator\n",
    "    B = 0 # Baseline accumulator - used primarily for the average baseline case\n",
    "    loss = Variable(torch.Tensor([0]))\n",
    "    if self.iscuda:\n",
    "        loss = loss.cuda()\n",
    "        R = R.cuda()\n",
    "\n",
    "    # ---- Task-specific error computation\n",
    "    for t in reversed(range(self.T)):\n",
    "        loss = loss + task_errs[t].sum()/batch_size\n",
    "\n",
    "    # --- REINFORCE loss based on T-1 transitions ----\n",
    "    # Note: This will automatically be ignored when self.T = 1\n",
    "    for t in reversed(range(self.T-1)):\n",
    "        R = R + rewards[t] # A one sample MC estimate of Q[t]\n",
    "        if t == self.T-2:\n",
    "            B += self.R_avg\n",
    "        B += self.R_avg_expert * self.reward_scale_expert\n",
    "        adv = R - B\n",
    "        # PG loss\n",
    "        loss_term_1 = - (log_probs[t]*self.reward_scale*Variable(adv, requires_grad=False)).sum()/batch_size \n",
    "        # Entropy loss, maximize entropy\n",
    "        loss_term_2 = - self.lambda_entropy*entropies[t].sum()/batch_size\n",
    "        loss = loss + loss_term_1 + loss_term_2\n",
    "\n",
    "    self.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm(self.policy.parameters(), 10)\n",
    "    self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_probs, rec_errs, rewards, entropies, decoded, values, visited_idxes, decoded_all, _ = agent.gather_trajectory(state, eval_opts=None, pano_maps=pano_maps, opts=opts)\n",
    "# Backward pass\n",
    "agent.update_policy(rewards, log_probs, rec_errs, entropies, values) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a state object to retrieve view corresponding to the max actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateObject():\n",
    "    # Images: (Batch_size x 3 x 512 x 512) giving the current batch of images.\n",
    "    def __init__(self, images):\n",
    "        self.images = images\n",
    "        self.batch_size = images.shape[0]\n",
    "        # Create a list to map from action index to views.\n",
    "        action_to_view = []\n",
    "        for i in range(16):\n",
    "            for j in range(16):\n",
    "                start_idx_x = i*32\n",
    "                end_idx_x = (i+1)*32-1\n",
    "                start_idx_y = j*32\n",
    "                end_idx_y = (j+1)*32-1\n",
    "                action_to_view.append(( (start_idx_x, end_idx_x), (start_idx_y, end_idx_y) ))\n",
    "        self.action_to_view = action_to_view\n",
    "    \n",
    "    # Given a set of action probabilities, get the images corresponding to the maximal action.\n",
    "    # actions: (batch_size x 256) tensor given the probabilities of selecting a given action.\n",
    "    def get_view(self, actions=None):\n",
    "        # If actions==None (ie. first time selecting an action), just return the upper left.\n",
    "        if actions == None:\n",
    "            return self.images[:, :, 0:32, 0:32], np.array([[0, 0] for _ in range(self.batch_size)])\n",
    "            \n",
    "        action_inds = np.argmax(actions, axis=1) # indexes of max prob action.\n",
    "        # List of the indices used to retrieve the subarray corresponding to the views chosen by the max action.\n",
    "        view_subarray_inds = [action_to_view[action_max] for action_max in action_inds]\n",
    "        view_locs = np.array([[action_max%32/16, action_max//32/16] for action_max in action_inds]) # THIS MIGHT BE WRONG. ORDER MIGHT BE WRONG OR THE 2D REPRESENTATION OF LOCATION MIGHT BE INADEQUATE.\n",
    "        # Iterate over subarray inds to generate binary masks.\n",
    "        mask = np.zeros([self.batch_size, 3, 512, 512], dtype=bool)\n",
    "        for i, inds in enumerate(view_subarray_inds):\n",
    "            mask[i, :, inds[0][0]:inds[0][1], inds[1][0]:inds[1][1]] = 1\n",
    "        views = self.images[mask].reshape([self.batch_size, 3, 32, 32])\n",
    "        return views, view_locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
